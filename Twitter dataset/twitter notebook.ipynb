{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "import torch\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dgl.nn import GraphConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/Users/martynazimek/Desktop/Thesis/thesis_social_networks/Twitter dataset/archive/data/twitter\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(edge_file):\n",
    "    G = nx.Graph()\n",
    "    edges = pd.read_csv(edge_file, sep=\" \", names=[\"id_1\", \"id_2\"], header=None)\n",
    "    G.add_edges_from(edges.values)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all .edges files in the dataset\n",
    "edge_files = [f for f in os.listdir(data_folder) if f.endswith('.edges')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first edges file for preview\n",
    "edge_path = os.path.join(data_folder, edge_files[0])\n",
    "edges = pd.read_csv(edge_path, sep=\" \", names=[\"id_1\", \"id_2\"], header=None)\n",
    "\n",
    "# Display\n",
    "print(\"Edges:\")\n",
    "print(edges.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this shows a connection between two tweeter users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load only one of the graphs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only one graph for analysis\n",
    "edge_path = os.path.join(data_folder, edge_files[0])\n",
    "G = load_graph(edge_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print basic graph statistics for this ego network \n",
    "print(f\"Graph Loaded: {edge_files[0]}\")\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes represent individual Twitter users in the ego network and edges represent connections (interactions or friendships) between users. So --> this ego network consists of 220 Twitter users and there are 5971 connections between them --> so they seem highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ego network\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(G, node_size=30, edge_color=\"gray\", with_labels=False)\n",
    "plt.title(f\"Ego Network Visualization: {edge_files[0]}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly clustered --> dense connection; some isolated nodes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all the graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all edge files\n",
    "graphs = [load_graph(os.path.join(data_folder, f)) for f in edge_files]\n",
    "\n",
    "# Combine graphs if needed (assuming it's an undirected graph)\n",
    "G_combined = nx.Graph()\n",
    "for G in graphs:\n",
    "    G_combined.add_edges_from(G.edges())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict_combined = dict(G_combined.degree())  # Compute degree for all nodes\n",
    "degree_series_combined = pd.Series(degree_dict_combined)  # Convert to Pandas Series\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Degree Distribution Summary (Combined Graph):\")\n",
    "print(degree_series_combined.describe())\n",
    "\n",
    "#plot degree distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(degree_series_combined, bins=50, color='skyblue', edgecolor='black', log=True)  # Log scale\n",
    "plt.xlabel(\"Degree (Number of Connections)\")\n",
    "plt.ylabel(\"Frequency (Log Scale)\")\n",
    "plt.title(\"Degree Distribution of Combined Twitter Network\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most nodes have low connectivity (1â€“50 connections) --> low-degree nodes\n",
    "A small number of nodes are highly connected --> high-degree nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "nx.draw(G_combined, node_size=20, edge_color=\"gray\", with_labels=False)\n",
    "plt.title(\"Combined Twitter Network Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The center of the network has many interconnected nodes, there are several small clusters on the edges, multiple ego networks interacting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combined Graph Statistics:\")\n",
    "print(f\"Total number of nodes: {G_combined.number_of_nodes()}\")\n",
    "print(f\"Total number of edges: {G_combined.number_of_edges()}\")\n",
    "print(f\"Graph Density: {nx.density(G_combined):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 2576 users, 34 774 connections between them --> connections > users so multiple connections \n",
    "most users are not connected to each other directly (low density) but through intermediaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature names\n",
    "featnames_path = os.path.join(data_folder, edge_files[0].replace(\".edges\", \".featnames\"))\n",
    "feat_names = pd.read_csv(featnames_path, sep=\" \", header=None)\n",
    "print(feat_names.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe features correspond to hashtags, interests or groups that users are part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all .featnames files in the dataset\n",
    "featname_files = [f for f in os.listdir(data_folder) if f.endswith('.featnames')]\n",
    "\n",
    "# Initialize a Counter to count feature occurrences\n",
    "feature_counter = Counter()\n",
    "\n",
    "# Read each .featnames file and count occurrences\n",
    "for file in featname_files:\n",
    "    featnames_path = os.path.join(data_folder, file)\n",
    "    \n",
    "    # Read feature names file\n",
    "    with open(featnames_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)  # Split ID and feature name\n",
    "            if len(parts) > 1:  # Ensure there is a feature name\n",
    "                feature_name = parts[1]\n",
    "                feature_counter[feature_name] += 1\n",
    "\n",
    "# Get the 5 most common feature names\n",
    "top_features = feature_counter.most_common(5)\n",
    "\n",
    "# Display results\n",
    "print(\"Top 5 Most Frequent Feature Names:\")\n",
    "for name, count in top_features:\n",
    "    print(f\"{name}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the most frequent feature appears only 19 times among 2,576 nodes suggests that features alone are unlikely to be the primary driver of connectivity --> edges not features determine the network structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node features\n",
    "feat_path = os.path.join(data_folder, edge_files[0].replace(\".edges\", \".feat\"))\n",
    "features = pd.read_csv(feat_path, sep=\" \", header=None)\n",
    "print(features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each row is different user and each column is a different feature\n",
    "the features are binary values which suggests that it indicated if the user posseses a specific feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = features.shape[0]\n",
    "num_features = features.shape[1] - 1  # First column is likely the node ID\n",
    "\n",
    "print(f\"Total Nodes with Features: {num_nodes}\")\n",
    "print(f\"Each Node has {num_features} Features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define High-Degree and Low-Degree Thresholds\n",
    "high_degree_threshold = degree_series_combined.quantile(0.90)  # Top 10% most connected nodes\n",
    "low_degree_threshold = degree_series_combined.quantile(0.10)   # Bottom 10% least connected nodes\n",
    "\n",
    "# Extract nodes based on their degree\n",
    "high_degree_nodes = degree_series_combined[degree_series_combined >= high_degree_threshold].index\n",
    "low_degree_nodes = degree_series_combined[degree_series_combined <= low_degree_threshold].index\n",
    "\n",
    "# Print results\n",
    "print(f\"High-degree Nodes: {len(high_degree_nodes)}\")\n",
    "print(f\"Low-degree Nodes: {len(low_degree_nodes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "close in size (256 vs 270) - balances comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of high-degree and low-degree nodes for visualization\n",
    "subgraph_nodes = list(high_degree_nodes)[:50] + list(low_degree_nodes)[:50]\n",
    "subG = G_combined.subgraph(subgraph_nodes)\n",
    "\n",
    "# Plot the subgraph\n",
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(subG, node_size=30, edge_color=\"gray\")\n",
    "plt.title(\"High-Degree vs. Low-Degree Nodes (Sample)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "high-degree nodes form a central cluster and  low-degree nodes are more scattered in the periphery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Twitter dataset folder\n",
    "data_folder = \"/Users/martynazimek/Desktop/Thesis/thesis_social_networks/Twitter dataset/archive/data/twitter\"\n",
    "\n",
    "# Find the first available edge and feature file\n",
    "edge_files = sorted([f for f in os.listdir(data_folder) if f.endswith('.edges')])\n",
    "feat_files = sorted([f for f in os.listdir(data_folder) if f.endswith('.feat')])\n",
    "\n",
    "if not edge_files or not feat_files:\n",
    "    raise ValueError(\"No valid .edges or .feat files found in the dataset folder.\")\n",
    "\n",
    "# Select the first available dataset\n",
    "edge_file = os.path.join(data_folder, edge_files[0])\n",
    "feat_file = os.path.join(data_folder, feat_files[0])\n",
    "\n",
    "print(f\"Using dataset: {edge_file} & {feat_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(edge_file, sample_size=100):\n",
    "    \"\"\"Loads a subset of the Twitter graph from an edge list file.\"\"\"\n",
    "    try:\n",
    "        # Load edges\n",
    "        edges = pd.read_csv(edge_file, sep=\" \", names=[\"id_1\", \"id_2\"], header=None)\n",
    "\n",
    "        # Debugging: Print original size\n",
    "        print(f\"Total edges in file: {len(edges)}\")\n",
    "\n",
    "        # Ensure correct sampling: Limit dataset size BEFORE using it in DGL\n",
    "        edges = edges.sample(n=min(sample_size, len(edges)), random_state=42)\n",
    "\n",
    "        # Debugging: Print after sampling\n",
    "        print(f\"Sampled edges: {len(edges)}\")\n",
    "\n",
    "        src, dst = edges[\"id_1\"].values, edges[\"id_2\"].values\n",
    "\n",
    "        # Create DGL graph\n",
    "        graph = dgl.graph((src, dst))\n",
    "        graph = dgl.add_self_loop(graph)  # Add self-loops to avoid issues in GNNs\n",
    "\n",
    "        print(f\"Graph loaded: {graph.num_nodes()} nodes, {graph.num_edges()} edges\")\n",
    "        return graph\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading graph: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(edge_file, sample_size=100):\n",
    "    \"\"\"Loads a subset of the Twitter graph from an edge list file.\"\"\"\n",
    "    try:\n",
    "        # Load edges\n",
    "        edges = pd.read_csv(edge_file, sep=\" \", names=[\"id_1\", \"id_2\"], header=None)\n",
    "\n",
    "        # Debugging: Print original size\n",
    "        print(f\"Total edges in file: {len(edges)}\")\n",
    "\n",
    "        # Ensure correct sampling: Limit dataset size BEFORE using it in DGL\n",
    "        edges = edges.sample(n=min(sample_size, len(edges)), random_state=42)\n",
    "\n",
    "        # Debugging: Print after sampling\n",
    "        print(f\"Sampled edges: {len(edges)}\")\n",
    "\n",
    "        src, dst = edges[\"id_1\"].values, edges[\"id_2\"].values\n",
    "\n",
    "        # Create DGL graph\n",
    "        graph = dgl.graph((src, dst))\n",
    "        graph = dgl.add_self_loop(graph)  # Add self-loops to avoid issues in GNNs\n",
    "\n",
    "        print(f\"Graph loaded: {graph.num_nodes()} nodes, {graph.num_edges()} edges\")\n",
    "        return graph\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading graph: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(feat_file, graph, max_nodes=100):\n",
    "    \"\"\"Loads node features and aligns them with the graph nodes (subset for large datasets).\"\"\"\n",
    "    try:\n",
    "        # Load features\n",
    "        features = pd.read_csv(feat_file, sep=\" \", header=None)\n",
    "\n",
    "        # Debugging: Print original feature size\n",
    "        print(f\"Total nodes in feature file: {len(features)}\")\n",
    "\n",
    "        # Limit dataset size for debugging\n",
    "        features = features.iloc[:max_nodes, :]\n",
    "\n",
    "        # Debugging: Print after sampling\n",
    "        print(f\"Sampled feature nodes: {len(features)}\")\n",
    "\n",
    "        node_ids = features.iloc[:, 0].values  # First column = Node IDs\n",
    "        feature_matrix = features.iloc[:, 1:].values  # Remaining columns = Features\n",
    "\n",
    "        # Convert features to dictionary\n",
    "        feature_dict = {node_id: feature_matrix[i] for i, node_id in enumerate(node_ids)}\n",
    "\n",
    "        # Align features with the graph nodes\n",
    "        aligned_features = []\n",
    "        for node in graph.nodes().numpy():\n",
    "            if node in feature_dict:\n",
    "                aligned_features.append(feature_dict[node])\n",
    "            else:\n",
    "                aligned_features.append(np.zeros(feature_matrix.shape[1]))  # Fill missing features with zeros\n",
    "\n",
    "        feature_tensor = torch.tensor(np.array(aligned_features), dtype=torch.float32)\n",
    "\n",
    "        print(f\"Features loaded: {feature_tensor.shape[0]} nodes, {feature_tensor.shape[1]} features\")\n",
    "        return feature_tensor\n",
    "\n",
    "    except Exception as e:\n",
    "        print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first 5 lines of the edge file\n",
    "with open(edge_file, \"r\") as f:\n",
    "    print(f.readlines()[:5])\n",
    "\n",
    "# Print first 5 lines of the feature file\n",
    "with open(feat_file, \"r\") as f:\n",
    "    print(\"Feature file sample:\", f.readlines()[:10])  # Check first 10 lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph = load_graph(edge_file, sample_size=100)\n",
    "\n",
    "#if graph:\n",
    "#    features = load_features(feat_file, graph, max_nodes=100)\n",
    "#    if features is not None:\n",
    "#        print(f\"Final Graph: {graph.num_nodes()} nodes, Feature Matrix Shape: {features.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
